import subprocess
import uuid
from pathlib import Path
import shutil

from huggingface_hub import snapshot_download

def predict(prompt: str):
    model_dir = snapshot_download(
        repo_id="OmniAvatar/OmniAvatar-1.3B",
        local_dir="pretrained_models/OmniAvatar-1.3B",
        local_dir_use_symlinks=False
    )

    print(f"âœ… æ¨¡å‹å·²ä¸‹è½½è‡³: {model_dir}")

    model_dir = snapshot_download(
        repo_id="Wan-AI/Wan2.1-T2V-1.3B",
        local_dir="pretrained_models/Wan2.1-T2V-1.3B",
        local_dir_use_symlinks=False
    )

    print(f"âœ… æ¨¡å‹å·²ä¸‹è½½è‡³: {model_dir}")

    model_dir = snapshot_download(
        repo_id="facebook/wav2vec2-base-960h",
        local_dir="pretrained_models/wav2vec2-base-960h",
        local_dir_use_symlinks=False
    )

    print(f"âœ… æ¨¡å‹å·²ä¸‹è½½è‡³: {model_dir}")
    
    # 2. å†™å…¥å¤åˆæ ¼å¼è¾“å…¥
    input_dir = Path("tmp_inputs")
    input_dir.mkdir(exist_ok=True)
    input_file = input_dir / f"{uuid.uuid4().hex}.txt"
    input_file.write_text(prompt.strip())  # ğŸ§  prompt æ˜¯å®Œæ•´çš„ä¸€è¡Œå«æ–‡æœ¬+å›¾åƒ+éŸ³é¢‘è·¯å¾„

    # 2. æ„å»ºå‘½ä»¤
    cmd = [
        "torchrun",
        "--standalone",
        "--nproc_per_node=1",
        "scripts/inference.py",
        "--config", "configs/inference_1.3B.yaml",
        "--input_file", str(input_file)
    ]

    # 3. è¿è¡Œæ¨ç†
    result = subprocess.run(cmd, capture_output=True, text=True)

    # 4. ä½ éœ€è¦æ ¹æ® inference.py çš„é€»è¾‘æ¥æå–è¾“å‡ºè·¯å¾„
    print("STDOUT:\n", result.stdout)
    print("STDERR:\n", result.stderr)

    # 5. å‡è®¾è¾“å‡ºå›¾åƒä¿å­˜åœ¨ output/ ç›®å½•
    output_path = Path("output")  # â† æŒ‰ç…§å®é™…è¾“å‡ºä½ç½®ä¿®æ”¹
    if output_path.exists():
        files = list(output_path.glob("*.png"))  # å‡è®¾ç”Ÿæˆçš„æ˜¯ PNG
        if files:
            return str(files[0])
    
    return "No output generated"
